from lola.params import LexicalParameters
from lola.model import Model, SufficientStatistics
import numpy as np


class IBM2(Model):

    def __init__(self, lex_parameters, dist_parameters):
        """

        :param dist_parameters:
        :param lex_parameters:
        """
        self._dist_parameters = dist_parameters
        self._lex_parameters = lex_parameters

    def likelihood(self, e_snt, f_snt, i, j):
        """
        Calculate p(a_j, f_1^m|e_0^l)
        :param e_snt: English sentence, represented by an np.array of integers
        :param f_snt: French sentence, represented by an np.array of integers
        :param i: The ith position in the English sentence
        :param j: The jth position in the French sentence
        :return: The likelihood associated with the above variables
        """
        m = 1.0/len(f_snt)
        lex_decision = self._lex_parameters.get(e_snt[i], f_snt[j])
        dist_decision = self._dist_parameters.get(len(e_snt), len(f_snt), i, j)
        return m * lex_decision * dist_decision

    def posterior(self, e_snt, f_snt, i, j):
        """
        Calculate p(a_j | f_1^m, e_0^l) up to a normalisation constant
        :param e_snt: English sentence, represented by an np.array of integers
        :param f_snt: French sentence, represented by an np.array of integers
        :param i: The ith position in the English sentence
        :param j: The jth position in the French sentence
        :return: The posterior associated with the above variables
        """
        lex_decision = self._lex_parameters.get(e_snt[i], f_snt[j])
        dist_decision = self._dist_parameters.get(len(e_snt), len(f_snt), i, j)
        return lex_decision * dist_decision

    def initialise(self, initialiser):
        """
        Initialise the lexical parameters with the optimized IBM1 parameters
        :param initialiser: A dictionary containing other models (already trained ones)
        :return: Initialised lexical parameters
        """
        if 'IBM1' in initialiser:
            # we are replacing our own lexical parameters, by those of an IBM1 which has already been optimised
            self._lex_parameters = initialiser['IBM1'].lexical_parameters()


class ExpectedCountsIBM2(SufficientStatistics):

    def __init__(self, e_max_len, f_max_len, e_vocab_size, f_vocab_size):
        self._dist_counts = VogelDistortionParameters(e_max_len, f_max_len, 0.0)
        self._lex_counts = LexicalParameters(e_vocab_size, f_vocab_size, 0.0)

    def observation(self, e_snt, f_snt, i, j, p):
        """
        We count a Lexical event, that is,
        French word at position j being generated by English word at position i
        with probability p
        :param e_snt: English sentence, represented by an np.array of integers
        :param f_snt: French sentence, represented by an np.array of integers
        :param i: The ith position in the English sentence
        :param j: The jth position in the French sentence
        :param p: Probability associated with p(f_j|e_i)
        :return: Added count to distortion and lexical counts
        """
        self._dist_counts.plus_equals(len(e_snt), len(f_snt), i, j, p)
        self._lex_counts.plus_equals(e_snt[i], f_snt[j], p)

    def make_model(self):
        """
        Maximise our independent models and reset the counts.
        :return: New IBM2
        """
        self._dist_counts.normalise()
        self._lex_counts.normalise()
        model = IBM2(self._lex_counts, self._dist_counts)
        self._dist_counts = VogelDistortionParameters(self._dist_counts.e_max_len(),
                                                      self._dist_counts.f_max_len(),
                                                      0.0)
        self._lex_counts = LexicalParameters(self._lex_counts.e_vocab_size(),
                                             self._lex_counts.f_vocab_size(),
                                             0.0)
        return model

class VogelDistortionParameters:

    def __init__(self, e_max_len, f_max_len, p):
        length = e_max_len + f_max_len + 1
        self._categorical = np.full((length, 1), p)
        self._e_max_len = e_max_len
        self._f_max_len = f_max_len

    def e_max_len(self):
        """
        :return: length of the longest English sentence in the corpus
        """
        return self._e_max_len

    def f_max_len(self):
        """
        :return: length of the longest French sentence in the corpus
        """
        return self._f_max_len

    def jump(self, l, m, i, j):
        """
        Calculate Vogel's distortion parameter
        :param l: Max length of English sentence
        :param m: Max length of French sentence
        :param i: ith position in the English sentence
        :param j: jth position in the French sentence
        :return: The jump: p(i- floor(j * l / m))
        """
        return i - np.floor(j * l / m)

    def index_jump(self, l, m, i, j):
        """
        Calculate the index in the np.array associated with the jump parameter (by shifting)
        :param l: Max length of English sentence
        :param m: Max length of French sentence
        :param i: ith position in the English sentence
        :param j: jth position in the French sentence
        :return: An integer representing the jump in the np.array
        """
        jump = self.jump(l, m, i, j)
        jump_index = jump + self._f_max_len
        return jump_index.astype(int)

    def get(self, l, m, i, j):
        """
        Get the value of the distortion parameter associated with the parameters
        that is, the jump: p(i- floor(j * l / m))
        :param l: Max length of English sentence
        :param m: Max length of French sentence
        :param i: ith position in the English sentence
        :param j: jth position in the French sentence
        :return: Value at the specified jump
        """
        index_jump = self.index_jump(l, m, i, j)
        return self._categorical[index_jump]

    def normalise(self):
        """
        Locally normalise the distortion parameter
        :return: Normalised categorical distribution
        """
        z = self._categorical.sum()
        self._categorical /= z

    def plus_equals(self, l, m, i, j, p):
        """

        :param l: Max length of English sentence
        :param m: Max length of French sentence
        :param i: ith position in the English sentence
        :param j: jth position in the French sentence
        :param p: The posterior probability
        :return: Updated distortion counts
        """
        index_jump = self.index_jump(l, m, i, j)
        updated = self._categorical[index_jump] + p
        print(updated)
        self._categorical[index_jump] = updated
        return updated
